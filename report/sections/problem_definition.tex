\section{Problem Definition}\label{sec:problem_def}
In many of the modern statistical learning problems, we are interested in first using a data-driven approach to posit a statistical model for the problem at hand, and then performing inference using this selected model. However, if we use the same dataset twice for both model selection and inference, classical inference methods would yield unreliable, and typically overly optimistic results \citep{hong2018overfitting}. Selective inference or post-selection inference deals with exactly this problem by developing inference procedures (for after model selection) that are statistically sound. Assuming that the data at hand are independently and identically distributed, one of the most straightforward approaches for selective inference is data splitting. The approach of data splitting, as its name suggests, randomly partitions the dataset into two subsets, with one of them used to select a model, and the other used to perform inference. Since the two datasets are independent by assumption, we can then apply any standard model selection and inference methods to conduct the analysis. Despite its simplicity, this approach has its drawbacks in a number of different settings. As an example, in the case where there are high influential points, having these points assigned to one subset may result in a vastly different inference output than if it were assigned to the other. Furthermore, in the case where we do not have many observations available, data splitting, which further reduces the dataset size used for both the selection and inference steps, may increase the level of uncertainty in our analysis.

To address these shortcomings of data splitting, \cite{leiner2022data} introduces data fission, which splits each observation in the dataset into two pieces through external randomness, each containing some information about the original observation. Data fission works as follows. Given a dataset $(X_i)_{i=1}^n \distiid \pi_\theta$, with $\pi_\theta$ some distribution whose parameter $\theta$ is of interest. With the parameter $\tau$ controlling the amount of information allocated to either part, data fission perturbs each $X_i$ to form $f_\tau(X_i)$ and $g_\tau(X_i)$ such that both parts contain information about $\theta$, and that there exists some function $h$ such that $X_i = h(f_\tau(X_i), g_\tau(X_i))$. Data fission also requires one of the following two conditions to hold. One can then use $\left(f_\tau(X_i)\right)_{i=1}^n$ for selection and $\left(g_\tau(X_i)\right)_{i=1}^n$ for inference.
\begin{itemize}
\item (P1): $f_\tau(X_i)$ and $g_\tau(X_i)$ are independent with known distributions (up to unknown $\theta$);
\item (P2): $f_\tau(X_i)$ has a known marginal distribution and $g_\tau(X_i)$ has a known conditional distribution given $f_\tau(X_i)$ (up to unknown $\theta$).
\end{itemize}

Here we provide an instance of data fission satisfying (P1). Suppose $X_i\distas \distNorm(\mu, \Sigma)$. Draw $Z_i\distas\distNorm(0,\Sigma)$. Then $f_\tau(X_i) = X_i + \tau Z_i \distas \distNorm(\mu, (1+\tau^2)\Sigma)$ and $g_\tau(X_i) = X_i-\tau^{-1}Z_i \distas \distNorm(\mu, (1+\tau^{-2})\Sigma)$, and $f_\tau(X_i) \indep g_\tau(X_i)$. $\tau\in(0,\infty)$ controls the level of information allocated to $f_\tau(X_i)$, with a larger $\tau$ indicating a less informative $f_\tau(X_i)$. In this particular case, a clear connection between data fission and data splitting can be drawn by looking at the amount of Fisher information allocated to the selection and inference stage. Specifically, by letting $a=\{\frac{1}{n}, \frac{2}{n}, \dots, 1\}$ be the proportion of observations allocated to the selection stage under data splitting, setting $a=\frac{1}{1+\tau^2}$ ensures that the amount of Fisher information allocated to each of the two stages are the same between the two methods. See \cref{eg:fisher} for a more detailed derivation. Therefore, data fission in the Gaussian case can be viewed as a continuous analog of data splitting, as $\tau$ is not restricted to the finite number of possible values of $a$.

Moving beyond the Gaussian case, \cite{leiner2022data} also provides a general data fission procedure that satisfies (P2) for distributions in the exponential family via conjugate prior and likelihood pairs. Specifically, by viewing the distribution of $X$, denoted $p_x$, as the prior distribution in the Bayesian framework, we can pick the distribution of $Z$ (our source of external randomness) such that $p_x$ is a conjugate prior for the likelihood function of $Z$. Note that in this case, both $X$ and $\tau$ would be a parameter for the density of $Z$. Then by setting $f_\tau(X)=Z$ and $g_\tau(X)=X$, both the marginal distribution of $f_\tau(X)$ and the conditional distribution of $g_\tau(X)$ given $f_\tau(X)$ would be tractable, thus satisfying (P2). The general form of the distributions of $f_\tau(X)$ and $g_\tau(X)\given f_\tau(X)$ can be found in \cref{thm:conjugacy}. We also provide an example where we apply \cref{thm:conjugacy} to Gamma distributed random variables in \cref{eg:continuous}.

\cite{leiner2022data} applies data fission to constructing selective confidence intervals (CIs) in fixed-design (generalized) linear models where the selection step performs variable selection. Exact CIs for the inference step are derived for Gaussian linear regression models, and asymptotic CIs are derived for generalized linear models. This report focuses on selective inference in fixed-design Gaussian linear regression, which we will have a more in-depth discussion on in \cref{sec:project}.