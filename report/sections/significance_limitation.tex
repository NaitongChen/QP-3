\section{Significance}
As mentioned in the previous section, data splitting may not be an ideal framework for selective inference. In fact, there has been a variety of alternative procedures developed. As an example, the idea of introducing external randomness to derive valid inference procedures rather than randomly splitting the data is explore in \cite{tian2018selective} and \cite{rasines2021splitting}. In the Gaussian example discussed in \cref{sec:problem_def}, their approach is equivalent to setting $f_\tau(X) = X+\tau Z$ and $g_\tau(X) = X$. The finite sample distribution of $g(X) \given f(X)$ is also known when the data are Gaussian. However, if we move beyond Gaussianity, the distribution of $g(X) \given f(X)$ is only known asymptotically. In these cases, the use of this asymptotic result may hinder the performance of selective inference, particularly in the small sample setting, which is one situation where data splitting struggles that we would like to address. Another approach developed in \cite{fithian2014optimal} is data carving. Data carving performs model selection using some fixed model selection procedure $S$ on the original dataset. By denoting the selected model as $S(X)$, we then conduct inference on $X$ conditioned on $S(X)$. Instead of injecting external randomness, data carving performs inference using the ``leftover information'' from the selection step obtained through conditioning. However, since the distribution of $X\given S(X)$ depends on the selection procedure $S$, practitioners are limited to choices of $S$ such that $X\given S(X)$ can be obtained either analytically or numerically. Examples include LASSO \citep{lee2016exact} and step-wise regression \citep{tibshirani2016exact}.

Data fission combines the advantages of both approaches. Similar to the approach in \cite{tian2018selective} and \cite{rasines2021splitting}, we create two slightly perturbed datasets, $f_\tau(X)$ and $g_\tau(X)$, both of which are of the same size of the original data, by injecting external randomness ($Z$) to the original data at hand. Here the distribution of $Z$ and the perturbations are carefully constructed so that both the finite sample marginal distribution of $f_\tau(X)$ and conditional distribution $g_\tau(X)\given f_\tau(X)$ are known analytically. Subsequently, we conduct inference under $g(X)\given f(X)$, which is similar in spirit to data carving in terms of the splitting of information. It is worth noting that, by cleverly leveraging the mechanism of conjugate distributions, which \cite{leiner2022data} terms ``conjugate prior reversal'', we can derive data fission procedures for many distributions in the rich exponential family. The conditional distribution used for inference is then available regardless of the choice of the selection algorithm, thus making data fission applicable to a much wider-range of problems than both of the approaches discussed.

\section{Limitations and challenges}
blah

%Instead of randomly picking data points to be used in either the selection or inference stage, data fission allows both stages to take advantage of some information from each of the observations at hand. At the same time, we do not reduce the number of observations for either stages of selective inference under the data fission framework. 
%
%\begin{itemize}
%\item paper does not discuss robustness of the method to the distribution assumptions
%\item experiments do not cover cases where fissioned data get transformed to follow a different distribution
%\item paper does not discuss how to choose tuning parameter (controlling amount of information split between $f(X)$ and $g(X)$)
%\item following the previous point, this paper does not discuss the relations between having a discrete vs. continuous tuning parameter (e.g. the two different ways of fissioning exponentially distributed data in Appendix B of \cite{leiner2022data})
%\item Proofs have missing terms, inaccurate notations.
%\item The discrete version of data fission for Gamma data may be incorrect.
%\end{itemize}
