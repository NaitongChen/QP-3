\section{Significance}
As mentioned in the previous section, data splitting may not always be an ideal framework for selective inference. As a result, there has been a variety of alternative procedures developed. For example, the idea of introducing external randomness to derive valid inference procedures rather than randomly partitioning the data is explored in \cite{tian2018selective} and \cite{rasines2021splitting}. In the Gaussian example discussed in \cref{sec:problem_def}, their approach is equivalent to setting $f_\tau(X) = X+\tau Z$ and $g_\tau(X) = X$. The finite sample distribution of $g_\tau(X) \given f_\tau(X)$ is also known when the data are Gaussian. However, if we move beyond Gaussianity, the distribution of $g_\tau(X) \given f_\tau(X)$ is only known asymptotically. In these cases, using this asymptotic result may hinder the performance of selective inference, particularly in the small sample setting, which is one situation where data splitting struggles that we would like to address. Another approach developed in \cite{fithian2014optimal} is data carving. Data carving performs model selection using some fixed model selection procedure $S$ on the original dataset. By denoting the selected model as $S(X)$, we then conduct inference on $X$ conditioned on $S(X)$. Instead of injecting external randomness, data carving performs inference using the ``leftover information'' from the selection step obtained through conditioning. However, since the distribution of $X\given S(X)$ depends on the selection procedure $S$, practitioners are limited to choices of $S$ such that $X\given S(X)$ can be obtained either analytically or numerically. Examples include LASSO \citep{lee2016exact} and step-wise regression \citep{tibshirani2016exact}.

Data fission combines the advantages of both approaches. Similar to the approach in \cite{tian2018selective} and \cite{rasines2021splitting}, we create two slightly perturbed datasets, $f_\tau(X)$ and $g_\tau(X)$, both of which are of the same size of the original data. This is done by injecting external randomness ($Z$) to the original data at hand. Here the distribution of $Z$ and the perturbations are carefully constructed so that both the finite sample marginal distribution of $f_\tau(X)$ and conditional distribution $g_\tau(X)\given f_\tau(X)$ are known analytically. Subsequently, we conduct inference under $g_\tau(X)\given f_\tau(X)$, which is similar in spirit to data carving in terms of taking advantage of the ``leftover information''. It is worth noting that, by cleverly leveraging the mechanism of conjugate distributions, which \cite{leiner2022data} terms ``conjugate prior reversal'', we can derive data fission procedures for many distributions in the rich exponential family. The conditional distribution used for inference is then available regardless of the choice of the selection algorithm, thus making data fission applicable to a much wider-range of problems than both of the other approaches discussed.

\section{Limitations and challenges}\label{sec:challenge}
Closer inspection of the two data fission examples given in \cref{sec:problem_def} reveals that both the marginal distribution of $f_\tau(X)$ and conditional distribution $f_\tau(X) \given g_\tau(X)$ depend on parameters of the distribution of the original data $X$. In fact, a general assumption of data fission is that the distribution of $X$ needs to be known. This may not be a realstic assumption for many cases in practice. Although \cite{leiner2022data} provides an asymptotic CI for Gaussian linear regression when only a consistent estimator of the variance rather than the true value is available, much of the questions regarding the robustness of data fission to the distribution assumption beyond the Gaussian case remain unexplored. Furthermore, data fission may lead to $f_\tau(X)$ and $g_\tau(X) \given f_\tau(X)$ being under different distribution families than that of $X$. As an example, in \cref{eg:continuous}, with gamma distributed $X$, we have $f_\tau(X)$ following a negative binomial distribution. At the same time, the density of $g_\tau(X)\given f_\tau(X)$ as a function of the original parameter of interest $\theta$ may be highly complex and potentially non-convex. This poses challenges in the inference stage, as discussed in Appendix A.5 of \cite{leiner2022data}. (We provide an instance of this problem in \cref{eg:gam_cont}.) Finally, we note that many data fission procedures will also result in cases where the distributions of $g_\tau(X)\given f_\tau(X)$ depend explicitly on the realized values of $Z$. We more closely explore the effect this has on selective inference in \cref{sec:project}.

Before concluding this section, we note an instance of imprecise notation as well as a possible mistake in \cite{leiner2022data}. Theorem 1 in \cite{leiner2022data} misses the base measure term $m(\cdot)$ in the density function of $X$ and mistakenly uses the raw value of $x$ rather than its natural parameter $\phi(x)$ in both the densities of $X$ and $Z$. Here $\phi$ is the natural parameter transformation function for the distribution of $Z$. For example, for $\distPoiss(\lambda), \phi(\lambda) = \log\lambda$. We present a corrected version of the proof in \cref{thm:conjugacy}. Although the final statement of the theorem remains the same, this notation impreciseness would likely result in incorrect applications of the theorem that may fail to lead us to the desired data fission procedure. Additionally, there seems to be an incorrect data fission procedure derived for gamma distributed data. In particular, the marginal density for $Z$ does not match that of \cref{thm:conjugacy}. We present the details in \cref{eg:discrete}. It is worth noting that even if the result presented in \cite{leiner2022data} were indeed correct, it is still unclear how we can modify the inference step to accommodate this data fission procedure, as the dimension of $Z$ is no longer necessarily the same as $X$.